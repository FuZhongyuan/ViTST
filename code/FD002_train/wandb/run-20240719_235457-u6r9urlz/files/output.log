
  0%|                                                                                                                                                      | 0/360 [00:00<?, ?it/s]Traceback (most recent call last):
  File "H:\dachang\ViTST-main\code\FD002_train\run_VisionTextCLS.py", line 584, in <module>
    acc, precision, recall, F1, auc, aupr, rmse, mape, mae = fine_tune_hf(
  File "H:\dachang\ViTST-main\code\FD002_train\run_VisionTextCLS.py", line 296, in fine_tune_hf
    train_results = trainer.train()
  File "D:\Anaconda3\envs\pytorch-gpu\lib\site-packages\transformers\trainer.py", line 1859, in train
    return inner_training_loop(
  File "D:\Anaconda3\envs\pytorch-gpu\lib\site-packages\transformers\trainer.py", line 2203, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "D:\Anaconda3\envs\pytorch-gpu\lib\site-packages\transformers\trainer.py", line 3138, in training_step
    loss = self.compute_loss(model, inputs)
  File "D:\Anaconda3\envs\pytorch-gpu\lib\site-packages\transformers\trainer.py", line 3161, in compute_loss
    outputs = model(**inputs)
  File "D:\Anaconda3\envs\pytorch-gpu\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "H:\dachang\ViTST-main\code\FD002_train\..\models\vision_text_dual_encoder\modeling_vision_text_dual_encoder.py", line 373, in forward
    text_outputs = self.text_model(
  File "D:\Anaconda3\envs\pytorch-gpu\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda3\envs\pytorch-gpu\lib\site-packages\transformers\models\roberta\modeling_roberta.py", line 828, in forward
    encoder_outputs = self.encoder(
  File "D:\Anaconda3\envs\pytorch-gpu\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda3\envs\pytorch-gpu\lib\site-packages\transformers\models\roberta\modeling_roberta.py", line 517, in forward
    layer_outputs = layer_module(
  File "D:\Anaconda3\envs\pytorch-gpu\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda3\envs\pytorch-gpu\lib\site-packages\transformers\models\roberta\modeling_roberta.py", line 448, in forward
    layer_output = apply_chunking_to_forward(
  File "D:\Anaconda3\envs\pytorch-gpu\lib\site-packages\transformers\pytorch_utils.py", line 237, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "D:\Anaconda3\envs\pytorch-gpu\lib\site-packages\transformers\models\roberta\modeling_roberta.py", line 460, in feed_forward_chunk
    intermediate_output = self.intermediate(attention_output)
  File "D:\Anaconda3\envs\pytorch-gpu\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda3\envs\pytorch-gpu\lib\site-packages\transformers\models\roberta\modeling_roberta.py", line 359, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "D:\Anaconda3\envs\pytorch-gpu\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda3\envs\pytorch-gpu\lib\site-packages\transformers\activations.py", line 78, in forward
    return self.act(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 4.00 GiB total capacity; 4.83 GiB already allocated; 0 bytes free; 5.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF